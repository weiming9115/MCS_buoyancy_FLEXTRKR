{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fdd3a1c-2db3-4c17-8446-a8650c837113",
   "metadata": {},
   "source": [
    "#### feature stats files writeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21476cb0-9411-45f6-b7d0-384215e71d3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63195832-ed7b-4673-ac0e-ea45b1a994d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72603e01-b556-4e7f-a733-08c8729025db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_tracks_phase(data_tracks, var_name):\n",
    "    \"\"\"\n",
    "    return area-related variables \n",
    "    \"\"\"\n",
    "    var_list = []\n",
    "    track_list = []\n",
    "    \n",
    "    for track in data_tracks.tracks.values:\n",
    "\n",
    "        track_list.append(track)\n",
    "        tmp = data_tracks.sel(tracks=track)\n",
    "        phase_list = [tmp.idt_ccs_init.values, tmp.idt_mcs_init.values, tmp.idt_mcs_grow.values, tmp.idt_mcs_mature.values,\n",
    "                      tmp.idt_mcs_decay.values, tmp.idt_mcs_end.values]\n",
    "\n",
    "        ds = xr.Dataset(data_vars=dict(var_null=(['mcs_phase'], tmp[var_name].sel(times=phase_list).values)),\n",
    "                        coords=dict(mcs_phase=(['mcs_phase'],['CCS','Init', 'Grow', 'Mature', 'Decay', 'End'])))\n",
    "\n",
    "        var_list.append(ds)\n",
    "\n",
    "    ds_xr = xr.concat(var_list, pd.Index(track_list, name='tracks')).rename_vars({'var_null':var_name})\n",
    "        \n",
    "    return ds_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e3d1c41-76a0-4f97-9979-3cdd55392d47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_tracks_BL_features(files):\n",
    "    \"\"\"\n",
    "    return tracks by BL_TOT bins (unit: K)\n",
    "    \"\"\"\n",
    "    \n",
    "    track_list = []\n",
    "    BL_features_list = []    \n",
    "        \n",
    "    for file in files:\n",
    "        \n",
    "        track_list.append(int(file.name[-11:-6])) # save track number \n",
    "        \n",
    "        tmp = xr.open_dataset(file)\n",
    "        \n",
    "        # Buoy. estimates\n",
    "        thetae_bl = tmp.thetae_bl\n",
    "        thetae_lt = tmp.thetae_lt\n",
    "        thetae_sat_lt = tmp.thetae_sat_lt\n",
    "        sp = tmp.SP/100 # hPa\n",
    "\n",
    "        delta_pl=sp-100-400\n",
    "        delta_pb=100\n",
    "        wb=(delta_pb/delta_pl)*np.log((delta_pl+delta_pb)/delta_pb)\n",
    "        wl=1-wb\n",
    "\n",
    "        Buoy_CAPE = wb*(thetae_bl - thetae_sat_lt)/thetae_sat_lt*340 # (K)\n",
    "        Buoy_SUBSAT = wl*(thetae_sat_lt - thetae_lt)/thetae_sat_lt*340 # (K)\n",
    "        Buoy_TOT = Buoy_CAPE - Buoy_SUBSAT  # (K)\n",
    "        \n",
    "        # calculate BL+ fraction and BL+ amplitude\n",
    "        BL_mcs = Buoy_TOT.where(tmp.cloudtracknumber_nomergesplit > 0)\n",
    "        BL_pos = BL_mcs.where(BL_mcs > 0, 0)\n",
    "        BL_pos_nan = BL_mcs.where(BL_mcs > 0, np.nan)\n",
    "        BL_pos_binary = BL_pos.where(BL_pos ==0, 1)\n",
    "        mcs_area = tmp.cloudtracknumber_nomergesplit.sum(('x','y'))\n",
    "        BL_pos_area = BL_pos_binary.sum(('x','y')).rename('BL_positive_area')\n",
    "        BL_pos_frac = (BL_pos_area/mcs_area).rename('BL_positive_fraction')\n",
    "        BL_tot_bp = BL_pos_nan.mean(('x','y')).rename('BL_TOT_bp')\n",
    "        \n",
    "        # BL estimates max/min\n",
    "        BL_tot_max = BL_mcs.max(('x','y')).rename('BL_TOT_max')\n",
    "        BL_tot_min = BL_mcs.max(('x','y')).rename('BL_TOT_min')\n",
    "        \n",
    "        # BL estimates, MCS grids\n",
    "        BL_tot_mcs = Buoy_TOT.where(tmp.cloudtracknumber_nomergesplit > 0).mean(('x','y')).rename('BL_TOT_mcs')\n",
    "        BL_cape_mcs = Buoy_CAPE.where(tmp.cloudtracknumber_nomergesplit > 0).mean(('x','y')).rename('BL_CAPE_mcs')\n",
    "        BL_subsat_mcs = Buoy_SUBSAT.where(tmp.cloudtracknumber_nomergesplit > 0).mean(('x','y')).rename('BL_SUBSAT_mcs')\n",
    "    \n",
    "        # BL estimates, non-MCS grids\n",
    "        BL_tot_ouside = Buoy_TOT.where(tmp.cloudtracknumber_nomergesplit == 0).mean(('x','y')).rename('BL_TOT_outside')\n",
    "        BL_cape_ouside = Buoy_CAPE.where(tmp.cloudtracknumber_nomergesplit == 0).mean(('x','y')).rename('BL_CAPE_outside')\n",
    "        BL_subsat_ouside = Buoy_SUBSAT.where(tmp.cloudtracknumber_nomergesplit == 0).mean(('x','y')).rename('BL_SUBSAT_outside')        \n",
    "        \n",
    "        # BL estimates, 5-deg. mean\n",
    "        BL_tot_5deg = Buoy_TOT.sel(x=slice(10,30),y=slice(10,30)).mean(('x','y')).rename('BL_TOT_5degmean')\n",
    "        BL_cape_5deg = Buoy_CAPE.sel(x=slice(10,30),y=slice(10,30)).mean(('x','y')).rename('BL_CAPE_5degmean')\n",
    "        BL_subsat_5deg = Buoy_SUBSAT.sel(x=slice(10,30),y=slice(10,30)).mean(('x','y')).rename('BL_SUBSAT_5degmean')\n",
    "        \n",
    "        # BL estimates, 3-deg. mean (PNNL)\n",
    "        BL_tot_3deg = Buoy_TOT.sel(x=slice(14,26),y=slice(14,26)).mean(('x','y')).rename('BL_TOT_3degmean')\n",
    "        BL_cape_3deg = Buoy_CAPE.sel(x=slice(14,26),y=slice(14,26)).mean(('x','y')).rename('BL_CAPE_3degmean')\n",
    "        BL_subsat_3deg = Buoy_SUBSAT.sel(x=slice(14,26),y=slice(14,26)).mean(('x','y')).rename('BL_SUBSAT_3degmean')\n",
    "        \n",
    "        # replace 0 by nan to avoid incorrectness when doing averaging\n",
    "        BL_pos_area = BL_pos_area.where(BL_pos_area > 0, np.nan)\n",
    "        BL_pos_frac = BL_pos_frac.where(BL_pos_frac > 0, np.nan)\n",
    "        \n",
    "        BL_features_list.append(xr.merge([BL_pos_area, BL_pos_frac, BL_tot_bp, BL_tot_mcs,\n",
    "                                         BL_cape_mcs, BL_subsat_mcs, BL_tot_ouside,\n",
    "                                         BL_cape_ouside, BL_subsat_ouside,\n",
    "                                         BL_tot_5deg, BL_cape_5deg, BL_subsat_5deg,\n",
    "                                         BL_tot_3deg, BL_cape_3deg, BL_subsat_3deg,\n",
    "                                         BL_tot_max, BL_tot_min]))\n",
    "    \n",
    "    BL_features_xr = xr.concat(BL_features_list, pd.Index(track_list, name='tracks'))\n",
    "    \n",
    "    return BL_features_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed622f9e-8bfa-4230-a478-49e83d349838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_tracks_precip_features(files):\n",
    "    \"\"\"\n",
    "    return tracks by BL_TOT bins (unit: K)\n",
    "    \"\"\"\n",
    "    \n",
    "    track_list = []\n",
    "    precip_features_list = []    \n",
    "    corr_coeff_temp = []\n",
    "        \n",
    "    for file in files:\n",
    "        \n",
    "        track_list.append(int(file.name[-11:-6])) # save track number \n",
    "        \n",
    "        tmp = xr.open_dataset(file)\n",
    "        # calculate precip amplitude within mcs\n",
    "        prec_mcs = tmp.precipitationCal.where(tmp.cloudtracknumber_nomergesplit > 0)\n",
    "        mtpr_mcs = tmp.mtpr.where(tmp.cloudtracknumber_nomergesplit > 0)\n",
    "        prec_amp_mcs = prec_mcs.mean(('x','y')).rename('precipitationCal_mcs')\n",
    "        mtpr_amp_mcs = mtpr_mcs.mean(('x','y')).rename('mtpr_mcs')\n",
    "        \n",
    "        try:\n",
    "            # estimate spatial correlation \n",
    "            corr_coeff = []\n",
    "            for phase in tmp.mcs_phase:\n",
    "                prec_era5 = prec_mcs.sel(mcs_phase=phase)\n",
    "                prec_gpm = mtpr_mcs.sel(mcs_phase=phase)\n",
    "                \n",
    "                x1 = np.where(np.isnan(prec_era5.values.ravel())==0)[0]\n",
    "                x2 = np.where(np.isnan(prec_era5.values.ravel())==0)[0]\n",
    "                idx = np.intersect1d(x1,x2)\n",
    "\n",
    "                stats = pearsonr(prec_era5.values.ravel()[idx], prec_gpm.values.ravel()[idx])\n",
    "                corr_coeff.append(stats[0]) # save correlation coefficient \n",
    "            corr_coeff_space = xr.Dataset(data_vars=dict(corr_coeff_space=(['mcs_phase'],np.asarray(corr_coeff))),\n",
    "                                     coords=dict(mcs_phase=(['mcs_phase'], tmp.mcs_phase.values)))\n",
    "\n",
    "            # estimate temporal correlation \n",
    "            corr_coeff_temp.append(pearsonr(prec_amp_mcs.values.ravel(), mtpr_amp_mcs.values.ravel())[0])\n",
    "\n",
    "            precip_features_list.append(xr.merge([prec_amp_mcs ,mtpr_amp_mcs,\n",
    "                                                  corr_coeff_space]))\n",
    "        except:\n",
    "            # if nan in dataset (one file in 2009...) \n",
    "            corr_coeff = [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan] # fill with nan for 6 phases\n",
    "            corr_coeff_space = xr.Dataset(data_vars=dict(corr_coeff_space=(['mcs_phase'],np.asarray(corr_coeff))),\n",
    "                                     coords=dict(mcs_phase=(['mcs_phase'], tmp.mcs_phase.values)))\n",
    "\n",
    "            # estimate temporal correlation \n",
    "            corr_coeff_temp.append(np.nan)\n",
    "\n",
    "            precip_features_list.append(xr.merge([prec_amp_mcs ,mtpr_amp_mcs,\n",
    "                                                  corr_coeff_space]))\n",
    "            \n",
    "            print('error file: {}'.format(file))\n",
    "    \n",
    "    corr_coeff_temp_xr = xr.Dataset(data_vars=dict(corr_coeff_temp=(['tracks'],np.asarray(corr_coeff_temp))),\n",
    "                                 coords=dict(tracks=(['tracks'], track_list)))\n",
    "    precip_features_xr = xr.concat(precip_features_list, pd.Index(track_list, name='tracks'))\n",
    "\n",
    "    # merge two datasets\n",
    "    precip_features_xr = xr.merge([precip_features_xr, corr_coeff_temp_xr])\n",
    "    \n",
    "    return precip_features_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9c376a6-a7a9-4e28-83d6-e77f49a7705b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing year: 2001\n",
      "processing year: 2002\n",
      "processing year: 2003\n",
      "processing year: 2004\n",
      "processing year: 2005\n",
      "processing year: 2006\n",
      "processing year: 2007\n",
      "processing year: 2008\n",
      "processing year: 2009\n",
      "error file: /scratch/wmtsai/temp_mcs/mcs_stats/envs_track/2009/tropics_extend/mcs_era5_3D_envs_2009.22054.LD.nc\n",
      "error file: /scratch/wmtsai/temp_mcs/mcs_stats/envs_track/2009/tropics_extend/mcs_era5_3D_envs_2009.30154.LD.nc\n",
      "processing year: 2010\n",
      "processing year: 2011\n",
      "processing year: 2012\n",
      "processing year: 2013\n",
      "processing year: 2014\n",
      "processing year: 2015\n",
      "error file: /scratch/wmtsai/temp_mcs/mcs_stats/envs_track/2015/tropics_extend/mcs_era5_3D_envs_2015.00694.LD.nc\n",
      "processing year: 2016\n",
      "processing year: 2017\n",
      "processing year: 2018\n",
      "processing year: 2019\n",
      "processing year: 2020\n",
      "CPU times: user 3h 35min 39s, sys: 20min 20s, total: 3h 56min\n",
      "Wall time: 4h 2min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "out_dir = Path('/scratch/wmtsai/temp_mcs/mcs_stats/mcs_tracks_non2mcs/tracks_area_mean')\n",
    "\n",
    "data_tracks_list = []\n",
    "\n",
    "year_list = np.arange(2001,2021)\n",
    "for year in year_list:\n",
    "\n",
    "    print('processing year: {}'.format(year))\n",
    "    # directory of the mcs_3dvars files\n",
    "    dir_envs_track = Path('/scratch/wmtsai/temp_mcs/mcs_stats/envs_track/{}/tropics_extend'.format(year))\n",
    "    files = sorted(list(dir_envs_track.glob('*.LD.nc')))\n",
    "\n",
    "    # load data_tracks \n",
    "    data_tracks = xr.open_dataset('/scratch/wmtsai/temp_mcs/mcs_stats/mcs_tracks_non2mcs/mcs_tracks_non2mcs_{}.tropics30NS.extend.nc'.format(year))\n",
    "    \n",
    "    # data_tracks time spend (1) from initial and mature (2) from mature to end\n",
    "    data_tracks['hours_ccs2init'] = data_tracks.idt_mcs_init - data_tracks.idt_ccs_init\n",
    "    data_tracks['hours_init2mat']  = data_tracks.idt_mcs_mature - data_tracks.idt_mcs_init\n",
    "    data_tracks['hours_mat2end']  = data_tracks.idt_mcs_end - data_tracks.idt_mcs_mature\n",
    "    \n",
    "    # write out BL_features dataset based on mcs_envs output\n",
    "    data_BL_features = data_tracks_BL_features(files)\n",
    "    data_precip_features = data_tracks_precip_features(files)\n",
    "    \n",
    "    # extract existing area-related variables\n",
    "    data_ccs_area = data_tracks_phase(data_tracks, var_name='ccs_area')\n",
    "    data_core_area = data_tracks_phase(data_tracks, var_name='core_area')\n",
    "    data_cold_area = data_tracks_phase(data_tracks, var_name='cold_area')\n",
    "    data_area_features = xr.merge([data_ccs_area, data_core_area, data_cold_area])\n",
    "    \n",
    "    corr_temp = data_precip_features.corr_coeff_temp\n",
    "    corr_coeff_space = data_precip_features.corr_coeff_space.mean('mcs_phase')\n",
    "\n",
    "    data_tracks_out = xr.merge([data_tracks['mcs_duration'],\n",
    "                                data_tracks['hours_init2mat'],\n",
    "                                data_tracks['hours_mat2end'],\n",
    "                                data_BL_features,\n",
    "                                data_precip_features,\n",
    "                                data_area_features]\n",
    "                                )\n",
    "    \n",
    "    data_tracks_out.to_netcdf(out_dir / 'featstats_tracks_non2mcs_{}.tropics30NS.extend.nc'.format(year))\n",
    "    \n",
    "    del data_tracks_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36403ce-6c9f-49a5-ab56-ed2ea48205a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_kernel",
   "language": "python",
   "name": "base_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
